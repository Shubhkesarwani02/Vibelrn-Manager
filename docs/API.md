# üé® Quick Reference - LLM & BullMQ Integration

## üìÅ Files Created/Modified

### ‚ú® New Files Created:
```
src/
‚îú‚îÄ‚îÄ services/
‚îÇ   ‚îú‚îÄ‚îÄ geminiService.ts    ‚≠ê Gemini API client & analysis logic
‚îÇ   ‚îî‚îÄ‚îÄ queueService.ts     ‚≠ê BullMQ helper functions
‚îÇ
‚îú‚îÄ‚îÄ workers.ts              ‚≠ê Unified worker runner script
‚îÇ
.env.example                ‚≠ê Environment variables template
LLM_INTEGRATION_GUIDE.md    ‚≠ê Complete documentation
DEVELOPMENT_CHECKLIST.md    ‚≠ê Testing & verification guide
```

### üîß Modified Files:
```
src/
‚îú‚îÄ‚îÄ jobs/
‚îÇ   ‚îî‚îÄ‚îÄ llmWorker.ts        ‚úèÔ∏è  Refactored to use geminiService
‚îÇ
‚îú‚îÄ‚îÄ controllers/
‚îÇ   ‚îî‚îÄ‚îÄ reviewController.ts ‚úèÔ∏è  Updated to use queueService
‚îÇ
package.json                ‚úèÔ∏è  Added worker scripts
```

---

## üöÄ Quick Start Commands

### Development Mode (Hot Reload):
```bash
# Terminal 1: API Server
npm run dev

# Terminal 2: Background Workers
npm run workers:dev
```

### Production Mode:
```bash
# Build first
npm run build

# Terminal 1: API Server
npm start

# Terminal 2: Background Workers
npm run workers:build
```

---

## üîë Key Concepts

### 1. **Service Layer Pattern**

**Before:**
```typescript
// Direct queue usage in controller
await llmQueue.add('job', data);
```

**After:**
```typescript
// Clean service layer
import { addBatchToLLMQueue } from './services/queueService';
await addBatchToLLMQueue(data);
```

### 2. **Separation of Concerns**

```
Controller     ‚Üí  Business logic (what to do)
Service        ‚Üí  Implementation (how to do it)
Worker         ‚Üí  Background processing (when to do it)
```

### 3. **Data Flow**

```
API Request
    ‚Üì
Controller (adds job to queue)
    ‚Üì
Returns response immediately ‚ö°
    ‚Üì
Worker picks up job (in background)
    ‚Üì
Processes with Gemini API
    ‚Üì
Updates database
```

---

## üì¶ Service APIs

### GeminiService

```typescript
import geminiService from './services/geminiService';

// Analyze a single review
const result = await geminiService.analyzeToneSentiment(
  "Great product!", 
  9
);
// Returns: { tone: 'positive', sentiment: 'satisfied' }

// Batch analysis
const results = await geminiService.analyzeBatch([
  { text: "Great!", stars: 9 },
  { text: "Bad quality", stars: 2 }
]);
```

### QueueService

```typescript
import { 
  addToLLMQueue,
  addBatchToLLMQueue,
  logAPIRequest,
  getQueueStats 
} from './services/queueService';

// Queue single review
await addToLLMQueue({ id: '1', text: 'Great!', stars: 9 });

// Queue multiple reviews (more efficient)
await addBatchToLLMQueue([
  { id: '1', text: 'Great!', stars: 9 },
  { id: '2', text: 'Poor', stars: 2 }
]);

// Log an API request
await logAPIRequest('GET /reviews/trends');

// Get queue statistics
const stats = await getQueueStats();
console.log(stats);
// Output:
// {
//   llmQueue: { waiting: 5, active: 2, completed: 100, failed: 1 },
//   logQueue: { waiting: 0, active: 1, completed: 50, failed: 0 }
// }
```

---

## üéØ Common Use Cases

### Use Case 1: Process New Reviews

```typescript
// In controller after creating review
const review = await prisma.reviewHistory.create({ data: {...} });

// Queue for LLM processing
await addToLLMQueue({
  id: review.id.toString(),
  text: review.text,
  stars: review.stars
});
```

### Use Case 2: Batch Process Missing Analyses

```typescript
// Find reviews needing analysis
const reviews = await prisma.reviewHistory.findMany({
  where: {
    OR: [
      { tone: null },
      { sentiment: null }
    ]
  }
});

// Queue all at once
const jobs = reviews.map(r => ({
  id: r.id.toString(),
  text: r.text!,
  stars: r.stars
}));

await addBatchToLLMQueue(jobs);
```

### Use Case 3: Monitor Queue Health

```typescript
// Get current queue status
const stats = await getQueueStats();

if (stats.llmQueue.failed > 10) {
  console.error('Too many failed LLM jobs!');
  // Send alert
}

if (stats.llmQueue.waiting > 100) {
  console.warn('Queue is backing up');
  // Scale workers
}
```

---

## üîç Monitoring & Debugging

### Check Worker Status
```bash
# Workers should show:
üöÄ Starting BullMQ Workers...
üîÑ Log worker started
ü§ñ LLM worker started
‚úÖ Workers are running and waiting for jobs...
```

### Watch Processing in Real-Time
```bash
# In worker terminal, you'll see:
ü§ñ Processing review 123...
ü§ñ Gemini analyzed review: tone=positive, sentiment=satisfied
‚úÖ Analyzed review 123: tone=positive, sentiment=satisfied
```

### Check Queue in Redis
```bash
redis-cli
> KEYS bull:*
> LLEN bull:llmQueue:wait
> LLEN bull:llmQueue:active
```

### Query Database
```sql
-- Recently analyzed reviews
SELECT id, text, tone, sentiment, updated_at
FROM ReviewHistory
WHERE tone IS NOT NULL
ORDER BY updated_at DESC
LIMIT 10;

-- Access logs
SELECT * FROM AccessLog
ORDER BY id DESC
LIMIT 10;
```

---

## ‚öôÔ∏è Configuration

### Environment Variables (.env)
```env
# Required
DATABASE_URL="postgresql://user:password@localhost:5432/vibelrn_db"
GEMINI_API_KEY="your_api_key_here"

# Optional (defaults shown)
REDIS_HOST="localhost"
REDIS_PORT="6379"
REDIS_PASSWORD=""
PORT="3000"
```

### Worker Concurrency
```typescript
// In jobs/llmWorker.ts
export const llmWorker = new Worker(
  'llmQueue',
  async (job) => { ... },
  {
    connection,
    concurrency: 5,  // ‚Üê Adjust based on load
  }
);
```

### Gemini Service Tuning
```typescript
// In services/geminiService.ts
private async callGeminiAPI(prompt: string) {
  // ...
  generationConfig: {
    temperature: 0.3,     // Lower = more consistent
    topK: 1,
    topP: 1,
    maxOutputTokens: 100, // Adjust for longer responses
  }
}
```

---

## üêõ Troubleshooting Quick Fixes

| Problem | Solution |
|---------|----------|
| Workers not starting | Check Redis connection, verify `.env` |
| Jobs not processing | Restart workers, check `getQueueStats()` |
| Gemini API errors | Verify API key, check quota |
| Database timeout | Check connection pool, verify `DATABASE_URL` |
| No tone/sentiment | Check worker logs, verify Gemini API key |

---

## üìä Architecture Benefits

‚úÖ **Scalability:** Workers can run on separate machines  
‚úÖ **Reliability:** Jobs survive server restarts (stored in Redis)  
‚úÖ **Performance:** API responds instantly, processing happens async  
‚úÖ **Maintainability:** Clean service layer, easy to test  
‚úÖ **Observability:** Centralized logging via logQueue  
‚úÖ **Flexibility:** Easy to add more worker types  

---

## üéì Learning Resources

- **BullMQ Docs:** https://docs.bullmq.io/
- **Gemini API Docs:** https://ai.google.dev/docs
- **Prisma Docs:** https://www.prisma.io/docs
- **Redis Docs:** https://redis.io/docs/

---

## ‚úÖ Quick Health Check

Run these commands to verify everything works:

```bash
# 1. Check Redis
redis-cli ping
# Expected: PONG

# 2. Test API
curl http://localhost:3000/reviews/trends
# Expected: JSON response with categories

# 3. Queue a test review
curl "http://localhost:3000/reviews?category_id=1"
# Expected: JSON with reviews, some queued for LLM

# 4. Check worker console
# Expected: Processing messages appearing

# 5. Verify database
psql -d vibelrn_db -c "SELECT COUNT(*) FROM AccessLog;"
# Expected: Growing number
```

---

**Status:** ‚úÖ **Implementation Complete & Ready to Use**

**Quick Start:** Run `npm run dev` + `npm run workers:dev` in separate terminals!
